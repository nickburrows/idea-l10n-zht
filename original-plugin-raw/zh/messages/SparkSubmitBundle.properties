action.BigDataTools.Deploy.Configure.text=创建 Spark 提交配置…
action.open.local.run.config=本地 Spark Submit
action.open.ssh.run.config=SSH Spark Submit
add.new.ssh.connection.label=添加 SSH 连接...
artifact.tooltip=要在集群上运行的可执行文件的路径
cluster.manager.kubernetes=Kubernetes
cluster.manager.local=局部
cluster.manager.mesos=Apache Mesos
cluster.manager.nomad=Nomad
cluster.manager.standalone=独立
cluster.manager.tooltip=服务器上配置的集群管理器。
cluster.manager.yarn=Hadoop YARN
cluster.status.loading=正在加载…
cluster.status.no.target=无目标
configuration.description=Spark Submit 配置
configuration.name=Spark Submit
configuration.name.cluster=集群
configuration.name.local=本地(已弃用)
configuration.name.ssh=SSH (已弃用)
configuration.options.add=其他自定义
configuration.options.add.title=添加提交选项
dialog.archives.title=选择归档文件
dialog.artifactPath.title=选择应用程序
dialog.driverClassPath.title=选择驱动程序类路径
dialog.driverLibraryPath.title=选择驱动程序库路径
dialog.files.title=选择文件
dialog.input.spark.command.description=在此处粘贴 spark-submit 命令以填写 Spark Submit 表单
dialog.input.spark.command.label=Spark 命令
dialog.input.spark.command.title=Spark 输入
dialog.jars.title=选择 Jar 文件
dialog.keytabFile.title=选择键表文件
dialog.message.failed.to.create.ssh.process=无法创建 SSH 进程
dialog.message.not.found=找不到 {0}
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARK_HOME 应设置为正确的文件夹。
dialog.message.specify.application=指定应用程序
dialog.propertiesFile.title=选择属性文件
dialog.pyfiles.title=选择 Python 文件
dialog.select.artifact.button.open.artifact.settings=工件设置
dialog.select.artifact.empty=当前项目中没有工件
dialog.select.artifact.link.open.artifact.settings=设置工件
dialog.select.artifact.title=选择依赖项工件
dialog.select.class.empty=在所选应用程序中找不到任何类
dialog.select.class.title=类名
dialog.sparkHomePath.title=选择 Spark 主目录
dialog.targetDirectory.title=选择目标目录
dialog.workDir.title=选择工作目录
edit.ssh.configuration=编辑 SSH 配置
error.ssh=Specify SSH Config
error.ssh.config=应指定 SSH 配置
error.target.config=选择远程目标
error.target.id=Select Remote ID
fun.search.process.text=进程 {0}
load.command.string=加载 spark-submit 命令
notification.group.sftpsparkfileupload=Spark 的 SFTP 文件上传
open.cluster.info.description=点击以打开文档
progress.text.upload.to.host=将 {0} 上传到主机…
receive.artifact.task=接收工件…
remote.target.ssh.remark=SSH 配置
row.final.command=结果提交命令
row.final.command.copy=复制 spark-submit 命令
row.final.command.hint=您可以复制或加载一个 spark-submit 命令，它会填入相应的字段
settings.additional.title=高级提交选项
settings.additional.verbose=打印额外的调试输出
settings.application=应用程序:
settings.application.arguments=运行实参:
settings.application.class.hint=--class 的您应用程序的主类(对于 Java / Scala 应用)。
settings.application.class.name=类:
settings.application.class.name.error.msg=请先指定应用程序
settings.application.hint=传递给主类的 main 方法的实参(如果有)。
settings.beforeShellScript=提交脚本前
settings.beforeShellScript.hint=在 Spark Submit 之前执行的脚本。例如，“source activate py36”
settings.cluster.manager=集群管理器:
settings.cluster.manager.proxy.user=代理用户:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user 提交应用程序时要模拟的用户。<br>此实参对 --principal/--keytab 没有影响。</html>
settings.cluster.manager.queue=队列:
settings.cluster.manager.queue.hint=--queue 要提交到的 YARN 队列(默认值: "default")。
settings.cluster.manager.supervise=启用监督
settings.cluster.manager.supervise.hint=--supervise 如果给定，则在驱动程序失败时重启驱动程序。
settings.dependencies.files=文件:
settings.dependencies.files.hint=--files 要放置在每个执行程序的工作目录中的逗号分隔的文件列表。可以通过 SparkFiles.get(fileName)访问执行器中这些文件的文件路径。
settings.dependencies.jars=Jar:
settings.dependencies.jars.hint=--jars 以逗号分隔的要包含在驱动程序和执行程序类路径中的 jar 列表。
settings.dependencies.python=Py 文件:
settings.dependencies.python.hint=--py-files 要添加到 Python 应用的 PYTHONPATH 的 .zip、.egg 或 .py 文件的逗号分隔列表。
settings.dependencies.title=依赖项
settings.deploy.mode=部署模式:
settings.deploy.mode.hint=<html>--deploy-mode 是在本地(“客户端”)还是在集群(“集群”)内的<br>其中一台工作进程机器上启动驱动程序。</html>
settings.deploymode.client=客户端
settings.deploymode.cluster=集群
settings.driver.class.path=驱动程序类路径:
settings.driver.class.path.hint=<html>--driver-class-path 传递给驱动程序的额外类路径条目<br>请注意，使用 --jars 添加的 jar 会自动包含在类路径中。</html>
settings.driver.cores=驱动程序核心:
settings.driver.cores.hint=--driver-cores 驱动程序使用的核心数，仅在集群模式下。
settings.driver.java.options=驱动程序 Java 选项:
settings.driver.java.options.hint=--driver-java-options 传递给驱动程序的额外 Java 选项。
settings.driver.library.path=驱动程序库路径:
settings.driver.library.path.hint=--driver-library-path 要传递给驱动程序的额外库路径条目。
settings.driver.memory=驱动程序内存:
settings.driver.memory.hint=--driver-memory 驱动程序的内存(例如 1000M、2G)。
settings.driver.title=驱动程序
settings.envParams=环境变量
settings.envParams.hint=附加环境变量
settings.executor.archives=归档:
settings.executor.archives.hint=<html>--archives 要提取到每个执行器的工作目录中的归档列表。</html>
settings.executor.cores=执行器核心:
settings.executor.cores.hint=<html>--executor-cores 每个执行器的核心数量<br>(默认值: 在 YARN 模式下为 1，在独立模式下的工作进程上为所有可用核心)。</html>
settings.executor.cores.total=执行器核心总数:
settings.executor.cores.total.hint=--total-executor-cores 所有执行器的核心总数。
settings.executor.memory=执行器内存:
settings.executor.memory.default=1G
settings.executor.memory.hint=--executor-memory 每个执行器的内存(例如: 1000M、2G)
settings.executor.number=执行器数量:
settings.executor.number.hint=<html>--num-executors 要启动的执行器数量<br>如果启用动态分配，则初始执行器数量将至少为该值。</html>
settings.executor.title=执行器
settings.integration.spark.monitoring=连接:
settings.integration.spark.monitoring.add=新增
settings.integration.title=Spark 监控集成
settings.isInteractive=交互式
settings.isInteractive.hint=以 shell 交互式模式运行执行命令
settings.kerberos.keytab=键表:
settings.kerberos.keytab.hint=<html>--keytab 包含上面指定主体的键表的文件的完整路径<br>此键表将通过 Secure Distributed Cache 复制到运行 Application Master 的节点，<br>用于定期更新登录票证和委托令牌。</html>
settings.kerberos.principal=主体:
settings.kerberos.principal.hint=--principal 用于登录 KDC，同时在安全 HDFS 上运行的主体。
settings.kerberos.title=Kerberos
settings.master=主:
settings.master.hint=--master spark://host:port, mesos://host:port, yarn, k8s://https://host:port，或本地(默认值: local[*])。
settings.maven.exclude.packages=排除软件包:
settings.maven.exclude.packages.hint=<html>--exclude-packages 在解析 --packages 中提供的依赖项时<br>为避免依赖关系冲突而要排除的 groupId:artifactId 列表。</html>
settings.maven.packages=软件包:
settings.maven.packages.hint=<html>--packages 要包含在驱动程序和执行器类路径中的 jar 的 Maven 坐标列表<br>将搜索本地 Maven 仓库，然后是 Maven 中央仓库和 --repositories 给出的任何其他远程仓库<br>坐标格式应为 groupId:artifactId:version。</html>
settings.maven.repositories=仓库:
settings.maven.repositories.hint=--repositories 附加远程存储库列表，用于搜索 --packages 给出的 Maven 坐标。
settings.maven.title=Maven
settings.run.target.config=远程目标:
settings.shell.title=Shell 选项
settings.shellExecutor=Shell 路径
settings.shellExecutor.hint=Shell 的路径。如果启用交互式模式或在设置提交脚本之前使用
settings.spark.config=配置:
settings.spark.config.hint=--conf Spark 配置属性。
settings.spark.home=Spark 主目录:
settings.spark.properties.file=属性文件:
settings.spark.properties.file.hint=<html>--properties-file 加载额外属性的文件路径。<br>如果未指定，将查找 conf/spark-defaults.conf。</html>
settings.spark.python.sdk=使用 Python 环境运行:
settings.spark.title=Spark 配置
settings.ssh.config=SSH 配置:
settings.ssh.error.msg=请先选择 SSH 配置
settings.ssh.error.title=文件选择器错误
settings.ssh.target.dir=目标上传目录:
settings.ssh.target.dir.hint=<html>输入远程主机上将上传所有本地文件的目录的路径。<br>如果所选文件已存在于此目录中，它们将被覆盖。</html>
settings.ssh.title=SFTP 选项
settings.url.artifact.name=IDEA 工件
settings.url.custom.name=自定义
settings.url.file.name=文件
settings.url.gcs.name=GC Storage
settings.url.gradle.artifact.name=Gradle 工件
settings.url.gradle.artifact.tooltip=Gradle 工件
settings.url.hdfs.name=HDFS
settings.url.s3.name=S3
settings.url.server.mock.desc=文件路径:
settings.url.server.name=服务器文件
settings.url.upload.name=上传文件
settings.url.upload.tooltip=上传本地文件
settings.url.web.name=远程
settings.workingDirectory=工作目录
setup.ssh.config=设置 SSH 配置
spark.submit.gutter.icon.tooltip=在 Spark 集群上运行
sparkhome.tooltip=Spark 目录
upload.file.title=将文件上传到远程
upload.files.error=上传文件时出现异常。{0}
upload.files.success=已成功上传 {0} 个文件
upload.files.through.sftp.to.spark.host=通过 SFTP 上传文件
upload.target.dir.is.not.found=找不到目标目录“{0}”
work.directory.tooltip=指向脚本调用位置