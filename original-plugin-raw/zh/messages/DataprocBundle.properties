action.add.job.title=提交作业
action.cancel.job.confirm.msg=是否取消“{0}”作业?
action.cancel.job.title=取消作业
action.clone.job.title=克隆作业
action.cluster.remove.confirm.msg=是否删除集群“{0}”?
action.cluster.start.confirm.msg=是否启动集群“{0}”?
action.cluster.terminate.confirm.msg=是否终止集群“{0}”?
action.confirm.title=确认
action.delete.job.confirm.msg=是否删除“{0}”作业?
action.delete.job.title=删除作业
action.open.stage.bucket=打开暂存存储桶
action.sftp=打开到节点的 SFTP
action.sftp.master.node=打开到主节点的 SFTP
action.ssh=打开到节点的 SSH
action.ssh.master.node=通过 SSH 连接到主节点
add.job.title=提交作业
add.new.submit.connection.label=添加 Dataproc 连接...
cell.execution.finished.msg=作业 "{0}" 已完成，状态为 {1}。
cell.execution.finished.title=Dataproc 作业
cluster.action.delete=删除集群
cluster.action.start=启动集群
cluster.action.stop=终止集群
cluster.info.config.autoscaling=自动扩缩:
cluster.info.config.master.node.desc=主节点:
cluster.info.config.metastore=Dataproc 元存储:
cluster.info.config.monitoring=完整性监控:
cluster.info.config.network=网络:
cluster.info.config.region=区域:
cluster.info.config.scheduled.deletion=定时删除:
cluster.info.config.secure.boot=安全启动:
cluster.info.config.vtpm=VTPM:
cluster.info.config.worker.node.desc=工作进程节点:
cluster.info.config.zone=可用区
cluster.info.image.created=创建时间:
cluster.info.image.version=映像版本:
cluster.info.internal.ip=仅限内部 IP:
cluster.info.optional.components=可选组件:
cluster.info.summary.name=名称:
cluster.info.summary.state=状态:
cluster.info.summary.state.details=状态详细信息:
cluster.info.summary.type=类型:
cluster.info.summary.uiid=集群 UUID:
cluster.tab.applications.title=应用程序
cluster.tab.info.title=信息
cluster.tab.jobs.title=作业
cluster.tab.name=集群
cluster.tab.vb.instances.title=VM 实例
datamanager.configuration=配置
datamanager.job.info=作业信息
datamanager.labels=标签
datamanager.properties=属性
datamanager.summary=汇总
dataproc.error=Dataproc 错误
dataproc.error.cluster.must.be.started=必须运行集群。
dataproc.toolwindow.title=GC Dataproc
default.gcs.connection.name=GC Dataproc 项目
emr.remove.linked.connections.title=Dataproc 连接
error.connection.is.not.found=未为 Dataproc 设置连接。请重新创建。
error.json.auth.limited.msg=仅当您使用 gcloud CLI 在 Dataproc 中进行身份验证时，此操作才可用
error.json.auth.limited.title=操作不可用
group.name.dataproc=GC Dataproc
info.value.off=关闭
instance.config.gpu.number=GPU 数量
instance.config.local.ssd=本地 SSD
instance.config.machineType=机器类型:
instance.config.primary.disk.size=主磁盘大小:
instance.config.primary.disk.type=主磁盘类型:
job.hadoop.title=Hadoop
job.hive.title=Hive
job.info.client.tags=客户端标记
job.info.cluster=集群:
job.info.continue.on.failure=失败时继续
job.info.elapsed.time=经过时间:
job.info.jobId=作业 ID:
job.info.jobUuid=作业 UUID:
job.info.max.restart.per.hour=每小时最大重启:
job.info.max.restart.per.hour.hint=如果您不想在作业失败时自动重启，请留空。
job.info.open.job.files=在 GCS 中显示作业文件夹
job.info.properties=属性
job.info.query.file=查询:
job.info.query.file.value=查询文件:
job.info.query.text.value=查询文本:
job.info.query.type=查询源:
job.info.single.file.hint=可以是带有 gs:// 前缀的 GCS 文件、集群上带有 hdfs:// 前缀的 HDFS 文件或集群上带有 file:// 前缀的本地文件
job.info.spark.additional.py.files=附加 Python 文件:
job.info.spark.additional.py.files.title=选择附加 Py 文件
job.info.spark.additional.r.files=附加 R 文件:
job.info.spark.additional.r.files.title=选择附加 R 文件
job.info.spark.archives=归档:
job.info.spark.archives.hint=归档文件已在 Spark 工作目录中提取。可以是带有 gs:// 前缀的 GCS 文件、集群上带有 hdfs:// 前缀的 HDFS 文件或集群上带有 file:// 前缀的本地文件。支持的文件类型包括: .jar、.tar、.tar.gz、.tgz、.zip。
job.info.spark.archives.title=选择归档
job.info.spark.args=实参:
job.info.spark.files=文件:
job.info.spark.jars=Jar:
job.info.spark.jars.hint=Jar 文件包含在 CLASSPATH 中。可以是带有 gs:// 前缀的 GCS 文件、集群上带有 hdfs:// 前缀的 HDFS 文件或集群上带有 file:// 前缀的本地文件。
job.info.spark.jars.title=JAR
job.info.spark.main.class=主类:
job.info.spark.main.py.file.title=选择主 Py 文件
job.info.spark.main.pyfile=主 Python 文件:
job.info.spark.main.r.file=主 R 文件:
job.info.spark.main.r.file.title=选择主 R 文件
job.info.start.date=开始日期:
job.info.status=状态:
job.info.status.details=状态详细信息:
job.info.type=作业类型:
job.label.block.title=标签
job.pig.title=Pig
job.presto.title=Presto
job.properties.block.title=属性
job.pyspark.title=PySpark
job.query.file.dialog.title=选择查询文件:
job.query.file.label=查询文件:
job.query.source.file=文件
job.query.source.text=文本
job.query.source.type=查询类型:
job.query.text.hint=要执行的查询
job.query.text.label=查询文本:
job.spark.r.title=SparkR
job.spark.sql.title=SparkSql
job.spark.title=Spark
job.state.active=有效
job.state.canceled=已取消
job.state.done=完成
job.state.failed=已失败
job.validation.file.archive={0} 必须为归档类型 .jar、.tar、.tar.gz、.tgz、.zip。
job.validation.file.fs={0} 必须为带有 gs://、hdfs:// 或 file:// 前缀的文件
metainfo.cluster.id=ID:
metainfo.cluster.name=名称:
metainfo.cluster.status=状态:
remote.target.emr.cluster.remark=Dataproc 集群
resolve.artifact.is.not.supported=不支持检测 {0} 的主类。
settings.application.class.name.error.msg=请先选择 jar 文件
task.init.ssh.perform.cli.command=正在执行 GCloud CLI 命令…
task.init.ssh.title=Dataproc CLI 执行