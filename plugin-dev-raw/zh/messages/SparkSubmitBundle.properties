action.BigDataTools.Deploy.Configure.text=建立 Spark 提交組態…
action.open.local.run.config=本地 Spark Submit
action.open.ssh.run.config=SSH Spark Submit
add.new.ssh.connection.label=新增 SSH 連線...
artifact.tooltip=要在集群上執行的可執行檔案的路徑
cluster.manager.kubernetes=Kubernetes
cluster.manager.local=區域
cluster.manager.mesos=Apache Mesos
cluster.manager.nomad=Nomad
cluster.manager.standalone=獨立
cluster.manager.tooltip=伺服器上組態的集群管理器。
cluster.manager.yarn=Hadoop YARN
cluster.status.loading=正在載入…
cluster.status.no.target=無目標
configuration.description=Spark Submit 組態
configuration.name=Spark Submit
configuration.name.cluster=集群
configuration.name.local=本地(已棄用)
configuration.name.ssh=SSH (已棄用)
configuration.options.add=其他自定義
configuration.options.add.title=新增提交選項
dialog.archives.title=選擇歸檔檔案
dialog.artifactPath.title=選擇應用程式
dialog.driverClassPath.title=選擇驅動程序類別路徑
dialog.driverLibraryPath.title=選擇驅動程序庫路徑
dialog.files.title=選擇檔案
dialog.input.spark.command.description=在此處貼上 spark-submit 指令以填寫 Spark Submit 表單
dialog.input.spark.command.label=Spark 指令
dialog.input.spark.command.title=Spark 輸入
dialog.jars.title=選擇 Jar 檔案
dialog.keytabFile.title=選擇鍵表檔案
dialog.message.failed.to.create.ssh.process=無法建立 SSH 程序
dialog.message.not.found=找不到 {0}
dialog.message.spark.home.should.be.set.to.correct.folder=$SPARK_HOME 應設定為正確的目錄。
dialog.message.specify.application=指定應用程式
dialog.propertiesFile.title=選擇屬性檔案
dialog.pyfiles.title=選擇 Python 檔案
dialog.select.artifact.button.open.artifact.settings=工件設定
dialog.select.artifact.empty=當前專案中沒有工件
dialog.select.artifact.link.open.artifact.settings=設定工件
dialog.select.artifact.title=選擇相依項工件
dialog.select.class.empty=在所選應用程式中找不到任何類別
dialog.select.class.title=類別名
dialog.sparkHomePath.title=選擇 Spark 主目錄
dialog.targetDirectory.title=選擇目標目錄
dialog.workDir.title=選擇工作目錄
edit.ssh.configuration=編輯 SSH 組態
error.ssh=Specify SSH Config
error.ssh.config=應指定 SSH 組態
error.target.config=選擇遠端目標
error.target.id=Select Remote ID
fun.search.process.text=程序 {0}
load.command.string=載入 spark-submit 指令
notification.group.sftpsparkfileupload=Spark 的 SFTP 檔案上傳
open.cluster.info.description=點擊以開啟文檔
progress.text.upload.to.host=將 {0} 上傳到主機…
receive.artifact.task=接收工件…
remote.target.ssh.remark=SSH 組態
row.final.command=結果提交指令
row.final.command.copy=複製 spark-submit 指令
row.final.command.hint=您可以複製或載入一個 spark-submit 指令，它會填入相應的欄位
settings.additional.title=進階提交選項
settings.additional.verbose=列印額外的偵錯輸出
settings.application=應用程式:
settings.application.arguments=執行實參:
settings.application.class.hint=--class 的您套用程序的主類別(對於 Java / Scala 套用)。
settings.application.class.name=類別:
settings.application.class.name.error.msg=請先指定應用程式
settings.application.hint=傳遞給主類別的 main 方法的實參(如果有)。
settings.beforeShellScript=提交腳本前
settings.beforeShellScript.hint=在 Spark Submit 之前執行的腳本。例如，“source activate py36”
settings.cluster.manager=集群管理器:
settings.cluster.manager.proxy.user=代理使用者:
settings.cluster.manager.proxy.user.hint=<html>--proxy-user 提交應用程式時要模擬的使用者。<br>此實參對 --principal/--keytab 沒有影響。</html>
settings.cluster.manager.queue=佇列:
settings.cluster.manager.queue.hint=--queue 要提交到的 YARN 佇列(預設值: "default")。
settings.cluster.manager.supervise=啟用監督
settings.cluster.manager.supervise.hint=--supervise 如果給定，則在驅動程序失敗時重啟驅動程序。
settings.dependencies.files=檔案:
settings.dependencies.files.hint=--files 要放置在每個執行程序的工作目錄中的逗號分隔的檔案列表。可以通過 SparkFiles.get(fileName)存取執行器中這些檔案的檔案路徑。
settings.dependencies.jars=Jar:
settings.dependencies.jars.hint=--jars 以逗號分隔的要包含在驅動程序和執行程序類別路徑中的 jar 列表。
settings.dependencies.python=Py 檔案:
settings.dependencies.python.hint=--py-files 要新增到 Python 套用的 PYTHONPATH 的 .zip、.egg 或 .py 檔案的逗號分隔列表。
settings.dependencies.title=相依項
settings.deploy.mode=部署模式:
settings.deploy.mode.hint=<html>--deploy-mode 是在本地(“客戶端”)還是在集群(“集群”)內的<br>其中一台工作程序機器上啟動驅動程序。</html>
settings.deploymode.client=客戶端
settings.deploymode.cluster=集群
settings.driver.class.path=驅動程序類別路徑:
settings.driver.class.path.hint=<html>--driver-class-path 傳遞給驅動程序的額外類別路徑條目<br>請注意，使用 --jars 新增的 jar 會自動包含在類別路徑中。</html>
settings.driver.cores=驅動程序核心:
settings.driver.cores.hint=--driver-cores 驅動程序使用的核心數，僅在集群模式下。
settings.driver.java.options=驅動程序 Java 選項:
settings.driver.java.options.hint=--driver-java-options 傳遞給驅動程序的額外 Java 選項。
settings.driver.library.path=驅動程序庫路徑:
settings.driver.library.path.hint=--driver-library-path 要傳遞給驅動程序的額外庫路徑條目。
settings.driver.memory=驅動程序記憶體:
settings.driver.memory.hint=--driver-memory 驅動程序的記憶體(例如 1000M、2G)。
settings.driver.title=驅動程序
settings.envParams=環境變數
settings.envParams.hint=附加環境變數
settings.executor.archives=歸檔:
settings.executor.archives.hint=<html>--archives 要提取到每個執行器的工作目錄中的歸檔列表。</html>
settings.executor.cores=執行器核心:
settings.executor.cores.hint=<html>--executor-cores 每個執行器的核心數量<br>(預設值: 在 YARN 模式下為 1，在獨立模式下的工作程序上為所有可用核心)。</html>
settings.executor.cores.total=執行器核心總數:
settings.executor.cores.total.hint=--total-executor-cores 所有執行器的核心總數。
settings.executor.memory=執行器記憶體:
settings.executor.memory.default=1G
settings.executor.memory.hint=--executor-memory 每個執行器的記憶體(例如: 1000M、2G)
settings.executor.number=執行器數量:
settings.executor.number.hint=<html>--num-executors 要啟動的執行器數量<br>如果啟用動態分配，則初始執行器數量將至少為該值。</html>
settings.executor.title=執行器
settings.integration.spark.monitoring=連線:
settings.integration.spark.monitoring.add=新增
settings.integration.title=Spark 監控整合
settings.isInteractive=互動
settings.isInteractive.hint=以 shell 互動模式執行執行指令
settings.kerberos.keytab=鍵表:
settings.kerberos.keytab.hint=<html>--keytab 包含上面指定主體的鍵表的檔案的完整路徑<br>此鍵表將通過 Secure Distributed Cache 複製到執行 Application Master 的節點，<br>用於定期更新登入票證和委託令牌。</html>
settings.kerberos.principal=主體:
settings.kerberos.principal.hint=--principal 用於登入 KDC，同時在安全 HDFS 上執行的主體。
settings.kerberos.title=Kerberos
settings.master=主:
settings.master.hint=--master spark://host:port, mesos://host:port, yarn, k8s://https://host:port，或本地(預設值: local[*])。
settings.maven.exclude.packages=排除軟體套件:
settings.maven.exclude.packages.hint=<html>--exclude-packages 在解析 --packages 中提供的相依項時<br>為避免相依關係衝突而要排除的 groupId:artifactId 列表。</html>
settings.maven.packages=軟體套件:
settings.maven.packages.hint=<html>--packages 要包含在驅動程序和執行器類別路徑中的 jar 的 Maven 坐標列表<br>將搜尋本地 Maven 儲存庫，然後是 Maven 中央儲存庫和 --repositories 給出的任何其他遠端儲存庫<br>坐標格式應為 groupId:artifactId:version。</html>
settings.maven.repositories=儲存庫:
settings.maven.repositories.hint=--repositories 附加遠端存儲庫列表，用於搜尋 --packages 給出的 Maven 坐標。
settings.maven.title=Maven
settings.run.target.config=遠端目標:
settings.shell.title=Shell 選項
settings.shellExecutor=Shell 路徑
settings.shellExecutor.hint=Shell 的路徑。如果啟用互動模式或在設定提交腳本之前使用
settings.spark.config=組態:
settings.spark.config.hint=--conf Spark 組態屬性。
settings.spark.home=Spark 主目錄:
settings.spark.properties.file=屬性檔案:
settings.spark.properties.file.hint=<html>--properties-file 載入額外屬性的檔案路徑。<br>如果未指定，將尋找 conf/spark-defaults.conf。</html>
settings.spark.python.sdk=使用 Python 環境執行:
settings.spark.title=Spark 組態
settings.ssh.config=SSH 組態:
settings.ssh.error.msg=請先選擇 SSH 組態
settings.ssh.error.title=檔案選擇器錯誤
settings.ssh.target.dir=目標上傳目錄:
settings.ssh.target.dir.hint=<html>輸入遠端主機上將上傳所有本地檔案的目錄的路徑。<br>如果所選檔案已存在於此目錄中，它們將被覆蓋。</html>
settings.ssh.title=SFTP 選項
settings.url.artifact.name=IDEA 工件
settings.url.custom.name=自訂
settings.url.file.name=檔案
settings.url.gcs.name=GC Storage
settings.url.gradle.artifact.name=Gradle 工件
settings.url.gradle.artifact.tooltip=Gradle 工件
settings.url.hdfs.name=HDFS
settings.url.s3.name=S3
settings.url.server.mock.desc=檔案路徑:
settings.url.server.name=伺服器檔案
settings.url.upload.name=上傳檔案
settings.url.upload.tooltip=上傳本地檔案
settings.url.web.name=遠端
settings.workingDirectory=工作目錄
setup.ssh.config=設定 SSH 組態
spark.submit.gutter.icon.tooltip=在 Spark 集群上執行
sparkhome.tooltip=Spark 目錄
upload.file.title=將檔案上傳到遠端
upload.files.error=上傳檔案時出現異常。{0}
upload.files.success=已成功上傳 {0} 個檔案
upload.files.through.sftp.to.spark.host=通過 SFTP 上傳檔案
upload.target.dir.is.not.found=找不到目標目錄“{0}”
work.directory.tooltip=指向腳本呼叫位置